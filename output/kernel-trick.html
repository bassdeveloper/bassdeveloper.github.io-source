
<!DOCTYPE html>
<html lang="english">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="./theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">




    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Rishabh Chakrabarti" />
<meta name="description" content="An insight into the Kernel-trick which allow higher dimension access/operability. Source: eric-kim.net" />
<meta name="keywords" content="Kernel-Trick, Kernel">
<meta property="og:site_name" content="Bassdeveloper's Blog"/>
<meta property="og:title" content="Kernel Based Methods"/>
<meta property="og:description" content="An insight into the Kernel-trick which allow higher dimension access/operability. Source: eric-kim.net"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./kernel-trick.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-01-21 13:28:00+01:00"/>
<meta property="article:modified_time" content="2017-01-21 13:28:00+01:00"/>
<meta property="article:author" content="./author/rishabh-chakrabarti.html">
<meta property="article:section" content="Learning"/>
<meta property="article:tag" content="Kernel-Trick"/>
<meta property="article:tag" content="Kernel"/>
<meta property="og:image" content="/images/RC.jpg">

  <title>Bassdeveloper's Blog &ndash; Kernel Based Methods</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="/images/RC.jpg" alt="Rishabh Chakrabarti" title="Rishabh Chakrabarti">
      </a>
      <h1><a href=".">Rishabh Chakrabarti</a></h1>

<p>Learning about Data</p>
      <nav>
        <ul class="list">

          <li><a href="https://bassdeveloper.github.io/category/learning.html" target="_blank">Learning</a></li>
          <li><a href="https://bassdeveloper.github.io/category/data-science.html" target="_blank">Data-Science</a></li>
          <li><a href="https://bassdeveloper.github.io/category/business.html" target="_blank">Business</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-facebook" href="https://www.facebook.com/Rishabh.Chakrabarti" target="_blank"><i class="fa fa-facebook"></i></a></li>
        <li><a class="sc-github" href="https://github.com/bassdeveloper/" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-google-plus" href="https://plus.google.com/100116978271306424838" target="_blank"><i class="fa fa-google-plus"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
    <h1 id="kernel-trick">Kernel Based Methods</h1>
    <p>
          Posted on Sat 21 January 2017 in <a href="./category/learning.html">Learning</a>


    </p>
  </header>


  <div>
    <h1>KERNEL?</h1>
<blockquote>
<p>Do Read : http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html</p>
<p>A Kernel Function <span class="math">\(K(\vec{v},\vec{w})\)</span> is a function <span class="math">\(K:\Re^N \times \Re^N \rightarrow \Re\)</span> that obeys certain mathematical properties.</p>
</blockquote>
<p>For an intermediate definition, think of the kernel function as one that computes a <strong>dot product</strong> between <span class="math">\(\vec{v}, \vec{w}\)</span>.</p>
<h2>Linear SVM, Binary Classification</h2>
<p>Suppose that we have a 2-class dataset <span class="math">\(D\)</span>, and wewish to train a classifier <span class="math">\(C\)</span> to predict the class labels of future data points. This is known as <code>binary classification</code> problem, and can be cast as <code>Yes/No</code> questions such as :</p>
<ul>
<li>
<p>Medicine :</p>
<blockquote>
<p>Given a patient's vital data, does the patient have a cold?</p>
</blockquote>
</li>
<li>
<p>Computer Vision :</p>
<blockquote>
<p>Does the image contain a person?</p>
</blockquote>
</li>
</ul>
<p>A popular off-the-shelf classifier is the <strong>Support Vector Machine (SVM)</strong>, so we will use this as our classification algorithm.</p>
<h3><em>Binary vs. Multiclass Classification</em></h3>
<p>Binary classifiers are often the focus due to their simpler presentation.</p>
<p>However, many problems inherently have more than 2 possible outcomes.</p>
<p>For instance,</p>
<blockquote>
<p>To train a face verification system that can detect the identity of a photograph from a pool of <span class="math">\(N\)</span> people (where $N \gt 2 $).</p>
</blockquote>
<p><strong>This sort of problem is known as a <code>Multiclass</code> classification problem.</strong></p>
<p>Most mathematical definitions are binary classifiers.</p>
<p>There are 2 main approaches to the <code>Multiclass</code> problem :</p>
<ol>
<li>Directly add a multiclass extension to a binary classifier.</li>
<li><strong>Pros</strong>: Provides a principled way of solving the muticlass problem.</li>
<li>
<p><strong>Cons</strong> : Multiclass extensions tend to be much more complicated than the original binary formulation. This may lead to significantly longer training and test procedures. Even worse, experimental results may not be that much better than ad-hoc methods like (2).</p>
</li>
<li>
<p>Combine multiple binary classifiers to create a 'mega' multi-multiclass classifier. 2 popular implementations of this idea are the <code>"One-versus-One" (OVO)</code> and <code>"One-versus-All" (OVA)</code> scheme.</p>
</li>
<li>
<p><strong>Pros</strong> : Simple idea, easy to implement, can be much faster than multiclass extensions. Some people suggest that empirical results tend to be on par with (1).</p>
</li>
<li><strong>Cons</strong> : Ultimately, it is an ad-hoc method for solving the multiclass problem - there may exist datasets for which OVO?OVA will perform poorly on, but general multiclass classifiers (1) perform well on.</li>
</ol>
<p>Recall that a Linear SVM finds a hyperplane <span class="math">\(\vec{w}\)</span> that best separates the data points in the training set by class label.</p>
<blockquote>
<p><span class="math">\(\vec{w}\)</span> is called the <strong>decision boundary</strong>, and cuts the space into 2 halves : one half for class '0', and the other half for class '1'.</p>
</blockquote>
<p>To classify a point <span class="math">\(x_i \epsilon X\)</span> (where <span class="math">\(X\)</span> is the dataset), we simply see which 'side' of <span class="math">\(\vec{w}\)</span> that <span class="math">\(x_i\)</span> lies.</p>
<p><strong>Note</strong>: This description only applies to binary classification problems.</p>
<p>A sample dataset :</p>
<p><img alt="Linear_sample" src="./assets/2017-01-22-Kernel-trick-5e30e.png"></p>
<p>Training and evaluating a linear SVM on this dataset yields the following decision boundary.
<img alt="SVM_linear" src="./assets/2017-01-22-Kernel-trick-ca976.png"></p>
<p>Because the data is easily linearly separable, the SVM is able to find a margin that perfectly separates the training data, which also generalizes very well to the test set. The hyperplane (a line in ) separates the space into two halves: points that live in the brownish region are classified as class '1', whereas points that live in the blueish region are classified as class '0'.</p>
<p><strong>Unfortunately, in practice we will not always encounter such well-behaved datasets.</strong></p>
<p>Let's take a look at a dataset that is not linearly separable.</p>
<h2>A Linearly Nonseparable Dataset</h2>
<p><img alt="Non_linear_dataset" src="./assets/Non_linear_dataset.png"></p>
<p>The above dataset is a synthetically generated dataset.</p>
<div class="highlight"><pre><span></span>==== Evaluating Random Classifier
== Accuracy: 0.45
             precision    recall  f1-score   support

          0       0.74      0.42      0.53       151
          1       0.23      0.55      0.33        49

avg / total       0.62      0.45      0.48       200

==== Finished Random Classifier (0.000 s)

==== Evaluating SVM (kernel=&#39;linear&#39;), 2-fold cross validation
    Parameters to be chosen through cross validation:
        C: [1.0, 10.0, 100.0, 1000.0, 10000.0]
== Best Params: {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 1.0}
== Best Score: 0.476666666667
== Accuracy: 0.445
             precision    recall  f1-score   support

          0       0.78      0.37      0.50       151
          1       0.26      0.67      0.37        49

avg / total       0.65      0.45      0.47       200

==== Finished Linear SVM (1.290 s)
</pre></div>


<p><img alt="Linear_Kernel" src="./assets/Linear_Kernel.png"></p>
<p>As we can see, the <em>RandomClassifier</em> and <em>Linear SVM</em> both perform poorly. It's always a bummer when our classifier is as bad as random guessing!</p>
<p>To get a geometric sense of the SVM's failure to cope with this dataset, see the above figure for the decision boundary.</p>
<h2>Dealing with Nonseparable Data</h2>
<p>Obviously, we would like to handle linearly nonseparable data - otherwise, SVMs wouldn't be very useful.</p>
<blockquote>
<p>In the SVM example, the primary obstacle is the constraint that the decision boundary be linear in the original feature space (here,<span class="math">\(\Re^2\)</span> ).</p>
</blockquote>
<p><em>However, this is not always the correct decision boundary to find. For instance, in Figure 4, a better decision boundary would be a circular decision boundary that separates the outer cyan ring from the inner red ring.</em></p>
<blockquote>
<p>Could we generalize the SVM formulation to explicitly discover decision boundaries with arbitrary shape?</p>
</blockquote>
<p>As it turns out, if you get into the nitty-gritty mathematical details of the SVM, the derivations assume that the decision boundary is a separating hyperplane <span class="math">\(\vec{w}\)</span>.</p>
<ul>
<li>
<p>I imagine there is a way to recast the SVM <em>optimization problem</em> such that a <em>more-general decision surface</em> can be found, but I'd bet that the resulting optimization would carry a significant computational burden when compared to the linear SVM formulation.</p>
</li>
<li>
<p>So, it appears that we are stuck with an SVM that, for an N-dimensional dataset, finds an (N-1)-dimensional separating hyperplane.</p>
</li>
</ul>
<p><strong>What if we could play with N...?</strong></p>
<h2>IDEA !</h2>
<p>Separable in a higher dimension</p>
<p><img alt="Higher_Dimension" src="./assets/Higher_Dimension.png"></p>
<p>Let's think outside the box for a moment.</p>
<p>Consider the linearly nonseparable dataset in the above figure (left), with its two concentric rings.</p>
<p>Imagine that this dataset is merely a 2-D version of the <em>'true'</em> dataset that lives in <span class="math">\(\Re^3\)</span>, the above figure (right).</p>
<p>The <span class="math">\(\Re^3\)</span> dataset is easily linearly separable by a hyperplane. Thus, provided that we work in this <span class="math">\(\Re^3\)</span> space, we can train a linear SVM classifier that <strong>successfully finds a good decision boundary</strong>.</p>
<p>However, we are given the dataset in <span class="math">\(\Re^2\)</span>.</p>
<blockquote>
<p>The challenge is to find a transformation <span class="math">\(T:\Re^2\)</span> -&gt; <span class="math">\(\Re^3\)</span>, such that the transformed dataset is linearly separable in <span class="math">\(\Re^3\)</span>.</p>
</blockquote>
<p>In the above figure, the <span class="math">\(T\)</span> used is :</p>
<div class="math">$$T([x_1,x_2])=[x_1,x_2,x_1^2+x_2^2]$$</div>
<p>which after applied to every point on the left, yields the linearly separable dateset on the right.</p>
<blockquote>
<p>Note : It is  a convention to use the Greek letter 'phi' <span class="math">\(\phi\)</span> for this transformation, so <span class="math">\(\phi\)</span> will be used from hereon.</p>
</blockquote>
<p>Assuming we have such a transformation <span class="math">\(\phi\)</span>, the new classification pipeline is as follows.</p>
<p>First transform the training set <span class="math">\(X\)</span> to <span class="math">\(X'\)</span> with <span class="math">\(\phi\)</span>. Train a linear SVM on <span class="math">\(X'\)</span> to get classifier <span class="math">\(f_{svm}\)</span>. At test time, a new example <span class="math">\(\vec{x}\)</span> will first be transformed to <span class="math">\(\vec{x'}=\phi(\vec{x})\)</span>. The output class label is then determined by :<span class="math">\(f_{svm}(\vec{x'})\)</span></p>
<blockquote>
<p>Observation : This is exactly the same as the train/test procedure for regular linear SVMs, but with an added data transformation via <span class="math">\(\phi\)</span>.</p>
</blockquote>
<p>In the next figure, note that the hyperplane learned in <span class="math">\(\Re^3\)</span> is non-linear when projected back to <span class="math">\(\Re^2\)</span>. Thus, we have improved the <strong>expressiveness</strong> of the Linear SVM classifier by working in a higher-dimnensional space.</p>
<p><img alt="Decision Boundary" src="./assets/2017-01-22-Kernel-trick-ecb81.png"></p>
<h4>Recap :</h4>
<p>A dataset <span class="math">\(D\)</span> that is not linearly separable in <span class="math">\(\Re^N\)</span> may be linearly separable in a higher-dimensional space <span class="math">\(\Re^M\)</span> (where <span class="math">\(M \gt N\)</span>). Thus, if we have a transformation <span class="math">\(\phi\)</span> that lifts the dataset <span class="math">\(D\)</span> to a higher-dimensional <span class="math">\(D'\)</span> <u>such that</u> <span class="math">\(D'\)</span>  is linearly separable, then we can train a linear SVM on <span class="math">\(D'\)</span> to find a decision boundary <span class="math">\(\vec{w}\)</span> that separates the classes in <span class="math">\(D'\)</span>. Projecting the decision boundary <span class="math">\(\vec{w}\)</span> found in <span class="math">\(\Re^M\)</span> back to the original space <span class="math">\(R^N\)</span> will yield a nonlinear decision boundary.</p>
<p>This means that we can learn nonlinear SVMs <strong>while still using the original Linear SVM formulation!</strong></p>
<h4>Caveat : Impractical for large dimensions</h4>
<p>The scheme described so far is attractive due to its simplicity : we only modify the inputs to a 'vanilla' linear SVM. However, consider the computational consequences of increasing the dimensionality from <span class="math">\(R^N\)</span> to <span class="math">\(R^M\)</span> (with <span class="math">\(M \gt N\)</span>). If <span class="math">\(M\)</span> grows very quickly with respect to <span class="math">\(N\)</span> (e.g. <span class="math">\(M\)</span> <span class="math">\(\epsilon\)</span> <span class="math">\(O (2^N)\)</span>), then learning SVMs via dataset transformations will incur serious computational and memory problems!</p>
<p>Here is a concrete example : the Polynomial Kernel is a kernel often used with SVMs. For a dataset in <span class="math">\(\Re^2\)</span> a two-degree polynomial kernel (implicitly) performs the transfromation <span class="math">\([x_1,x_2]=[x_1^2,x_2^2,\sqrt{2}\cdot x_1 \cdot x_2, \sqrt{2 \cdot c}\cdot x_2, c]\)</span>.</p>
<p>This transformation adds three additional dimensions <span class="math">\(\Re^2\)</span> -&gt; <span class="math">\(\Re^5\)</span>.</p>
<p>In general, a d-dimensional polynomial kernel maps from <span class="math">\(R^N\)</span> to an <span class="math">\(\binom{N+d}{d}\)</span>- dimensional space. Thus, for datasets with large dimensionality, naively performing such a transformation will quickly become intractable.</p>
<h3>We only need the dot products!</h3>
<p>It turns out that the SVM has no need to explicitly work in the higher-dimensional space at training or testing time.</p>
<p>One can show that during training, the optimization problem only uses the training examples to compute <strong>pair-wise</strong> <em>dot products</em> <span class="math">\(&lt;\vec{x_i},\vec{x_j}&gt;\)</span>, where <span class="math">\(\vec{x_i},\vec{x_j}\)</span>  <span class="math">\(\epsilon\)</span> <span class="math">\(\Re^N\)</span>.</p>
<h4>Why is this significant?</h4>
<blockquote>
<p>It turns out that there exist functions that, given 2 vectors <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span> in <span class="math">\(\Re^N\)</span>, implicitly computes the dot product between <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span> in <span class="math">\(\Re^M\)</span> **without explicitly transforming <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span> to <span class="math">\(\Re^M\)</span>.</p>
</blockquote>
<h5>Such functions are called <strong>kernel</strong> functions <span class="math">\(K(\vec{v},\vec{w})\)</span>.</h5>
<p>The implications are:</p>
<ol>
<li>
<p>By using a kernel <span class="math">\(K(\vec{x_i},\vec{x_j})\)</span>, we can implicitly transform datasets to a higher-dimensional <span class="math">\(\Re^M\)</span> using no extra memory, and with a minimal effect on computation time.</p>
</li>
<li>
<p>The only effect on computation is the extra time required to compute <span class="math">\(K(\vec{x_i},\vec{x_j})\)</span>. Depending on <span class="math">\(K\)</span>, this can be minimal.</p>
</li>
<li>
<p>By virtue of (1), we can efficiently learn non-linear decision boundaries for SVMs simply by **replacing al dot products in the SVM computation with <span class="math">\(K(\vec{x_i},\vec{x_j})\)</span>.</p>
</li>
</ol>
<p><strong>The usage of kernel functions to achieve benefits (1) and (2) is the "Trick" in the "Kernel Trick"</strong></p>
<h3>Kernel functions,</h3>
<p>In this context, a Kernel function is a function $ K : \Re^N \times \Re^N \rightarrow \Re $. There are some important mathematical properties that must be obeyed in order to be considered a proper kernel function.</p>
<h4>Intuition</h4>
<p>A kernel <span class="math">\(K\)</span> effetively computes dot products in a higher-dimensional space <span class="math">\(R^M\)</span> while remaining in <span class="math">\(R^N\)</span>.</p>
<p>In symbols :</p>
<p>For <span class="math">\(\vec{x_i},\vec{x_j}\)</span>  <span class="math">\(\epsilon\)</span> <span class="math">\(\Re^N\)</span>, <span class="math">\(K(\vec{x_i},\vec{x_j})={&lt;\Phi(\vec{x_i}),\Phi(\vec{x_j})&gt;}_M\)</span>, where <span class="math">\({&lt;\cdot,\cdot&gt;}_M\)</span> is an inner product of <span class="math">\(\Re^M\)</span>, <span class="math">\(M \gt N\)</span>, and <span class="math">\(\Phi(\vec{x})\)</span> transforms <span class="math">\(\vec{x}\)</span> to <span class="math">\(\Re^M(\Phi : \Re^N \rightarrow \Re^M)\)</span></p>
<blockquote>
<p>The surprise here being the fact that the Kernel Function can compute dot products between <span class="math">\(\vec{v},\vec{w} \in \Re^N\)</span> in <span class="math">\(\Re^M\)</span> with a function <span class="math">\(K\)</span> that works exclusively in <span class="math">\(R^N\)</span>.</p>
</blockquote>
<h3>Popular Kernels</h3>
<p>Most off-the-shelf classifiers allow the user to specify one of three popular kernels: the <strong>polynomial,radial basis function</strong>, and <strong>sigmoid</strong> kernel.</p>
<p>For instance, sklearn's SVM implementation <code>svm</code>. SVC has a kernel parameter which can take on <code>linear</code>,<code>poly</code>,<code>rbf</code> or <code>sigmoid</code>.</p>
<p>You can even pass a custom kernel.</p>
<p>For the following, let <span class="math">\(\vec{x_i},\vec{x_j} \in \Re^N\)</span> be rows from the dataset <span class="math">\(X\)</span>.</p>
<ol>
<li>
<p><strong>Polynomial Kernel</strong> : <div class="math">$${(\gamma \cdot &lt;\vec{x_i},\vec{x_j}&gt; + r)}^d$$</div>
</p>
</li>
<li>
<p><strong>Radial Basis Function (RBF) Kernel</strong>: <div class="math">$$exp(- \gamma \cdot {|\vec{x_i} - \vec{x_j}|}^2 )$$</div>, where <span class="math">\(\gamma \gt 0\)</span>.</p>
</li>
<li>
<p><strong>Sigmoid Kernel</strong> :</p>
</li>
</ol>
<div class="math">$$tanh{(&lt;\vec{x_i},\vec{x_j}&gt; + r)}^d$$</div>
<blockquote>
<p>Note : For some reason, sklearn's <code>svm.SVC</code> appears to use both the <code>gamma</code> and <code>coef0</code> parameters for the <code>kernel = sigmoid</code>, despite the above definition only having one parameter <code>r</code>. Not cross-validating across both <code>gamma</code> and <code>coef0</code> resulted in degenerate decision boundaries.</p>
</blockquote>
<p><strong>Choosing the <code>correct kernel</code> is a <code>non-trivial</code> task.</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/kernel-trick.html">Kernel-Trick</a>
      <a href="./tag/kernel.html">Kernel</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Rishabh Chakrabarti </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Bassdeveloper's Blog ",
  "url" : ".",
  "image": "/images/RC.jpg",
  "description": "Rishabh Chakrabarti's Notes and Highlights"
}
</script>
</body>
</html>