{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Preprocessing Template\n",
    " Data pre-processing is an essential step in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "import numpy as np # Mathematics (Linear Algebra). Makes Python programming like R.\n",
    "import matplotlib.pyplot as plt # For plotting and viewing graphs from datasets\n",
    "import pandas as pd # Importing and managing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n",
      "Stored 'dataset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Data.csv') # Read the dataset and store into the 'dataset' variable\n",
    "print(dataset) # Lets take a look at the dataset\n",
    "%store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The X variable (dataset) predictor variables \n",
      "\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "The Y variable (dataset) \n",
      "\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Create Matrix of Features\n",
    "#%store -r dataset\n",
    "X = dataset.iloc[:, :-1].values # All columns except the last one\n",
    "print(\"\\nThe X variable (dataset) predictor variables \\n\")\n",
    "print(X)\n",
    "y = dataset.iloc[:, 3].values\n",
    "print(\"\\nThe Y variable (dataset) \\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix missing values\n",
    "from sklearn.preprocessing import Imputer # Imputer class helps in data preprocessing. \n",
    "# help(Imputer)# Imputation transformer for completing missing values\n",
    "# Here, we are replacing 'NaN' (missing values) with mean\n",
    "imputer=Imputer(missing_values='NaN', strategy='mean',axis=0) # Mean is default value. \n",
    "imputer.fit(X[:,1:3])\n",
    "X[:,1:3]=imputer.transform(X[:,1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is X after Label Encoding\n",
      "\n",
      "[[  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.40000000e+01\n",
      "    7.20000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   2.70000000e+01\n",
      "    4.80000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.00000000e+01\n",
      "    5.40000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.80000000e+01\n",
      "    6.10000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   4.00000000e+01\n",
      "    6.37777778e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.50000000e+01\n",
      "    5.80000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.87777778e+01\n",
      "    5.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.80000000e+01\n",
      "    7.90000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "    8.30000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.70000000e+01\n",
      "    6.70000000e+04]]\n",
      "\n",
      "\n",
      "This is y after Label Encoding\n",
      "\n",
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert Categorical Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X= LabelEncoder()\n",
    "X[:,0]=labelencoder_X.fit_transform(X[:,0])\n",
    "print(\"This is X after Label Encoding\\n\")\n",
    "print(X)\n",
    "labelencoder_y=LabelEncoder()\n",
    "y=labelencoder_X.fit_transform(y)\n",
    "print(\"\\n\\nThis is y after Label Encoding\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The main problem here is that machine learning is based on equations and numbers. The categorical variables are given an ordinal value and this means that the model can be biased for the higher value. i.e. \n",
    "\n",
    "* 0 for France\n",
    "* 1 for Germany\n",
    "* 2 for Spain\n",
    "\n",
    "could mean that Spain gets a higher priority than `France` or `Germany` and this could lead to an invalid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.40000000e+01,   7.20000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.70000000e+01,   4.80000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          3.00000000e+01,   5.40000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.80000000e+01,   6.10000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          4.00000000e+01,   6.37777778e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.50000000e+01,   5.80000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.87777778e+01,   5.20000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.80000000e+01,   7.90000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          5.00000000e+01,   8.30000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.70000000e+01,   6.70000000e+04]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder=OneHotEncoder(categorical_features=[0])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting test set and trainset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Feature scaling is a very important method. If variables are not on the same scale, it will cause issues in the machine learning model. Why is it so?\n",
    "\n",
    "Since a lot of machine learning models are based on **Euclidean Distance**. This means that they work on the squared differences. If the variable is not scaled, it would mean that the difference between one squared difference and another would be large. Thus, scaling is needed.\n",
    "\n",
    "There are two types of feature scaling:\n",
    "1. **Standardization**\n",
    "\n",
    "$$x_{stand}=\\frac{x-\\bar{x}}{{\\sigma}_x}$$\n",
    "\n",
    "Where ${\\sigma}_x$ is the Standard Deviation and $\\bar{x}$ is the mean of x.\n",
    "\n",
    "2. **Normalization**\n",
    "\n",
    "$$ x_{norm}=\\frac{x-min(x)}{max(x)-min(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train=sc_X.fit_transform(X_train)\n",
    "X_test=sc_X.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
