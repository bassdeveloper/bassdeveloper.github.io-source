Title: Support Vector Machines
Date: 2017-01-17 11:28
Modified: 2017-01-20 13:15
Category: Learning
Tags: SVM, Linear-Kernels
Slug: svm
Authors: Rishabh Chakrabarti
Summary: Support Vector Machines learnt from the [SVM Tutorial](http://www.svm-tutorial.com/), by Alexandre KOWALCZYK.

# SVM
> Do visit [SVM Tutorial website](http://www.svm-tutorial.com/).


> The main goal in SVM is to design a hyperplane that classifies all training vectors in 2 classes.

* **SVM needs training data thus comes under supervised learning**.
* **SVM is a classification algorithm**

### Example :
![SVM_Training]({filename}/assets/2017-01-17-SVM-e7301.png)

* We have plotted the size and weight of several people, and there is also a way to distinguish between men and women.

With such data, using a SVM will allow us to answer the following question:

> Given a particular data point (weight and size), is the person a man or a woman ?

* For instance:  if someone measures 175 cm and weights 80 kg, is it a man of a woman?



## Hyperplane

In geometry, a hyperplane is a **subspace of one dimension less than its ambient space**.

Example :

  * If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while

  * If the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.

This notion can be used in any general space in which the concept of the dimension of a subspace is defined.

##  Maximum margin?

Now the following problem exists :

> There can be 'n' number of hyperplanes that can be drawn that separate both our classes.

**But SVM's goal is to get the hyperplane that leaves the maximum margin from both classes ( i.e. the best choice )**

Thus, we arrive at the term : `Maximum Margin Hyperplane`.

![MarginA]({filename}/assets/2017-01-17-SVM-c2025.png)

**The Optimal HyperPlane is a no man's land.**

* **This means that the optimal hyperplane will be the one with the biggest margin.**

*That is why the objective of the SVM is to find  the optimal separating hyperplane which maximizes the margin of the training data.*

Equation for the hyperplane :
$$g(\vec{x}) = w^T\vec{x}+w_0$$

## How to Calculate this margin?

To do so, we need certain pre-requisites:

> SVM = Support VECTOR Machine

### What is a vector ?

If we define a point $A(3,4)$ in $\Re^2$, we can plot it like this :

![2D_point]({filename}/assets/2017-01-17-SVM-6c98b.png)

> Definition: Any point $x=(x1,x2),x\ne0$, in $\Re^2$,$\Re^2$ specifies a vector in the plane, namely the vector starting at the origin and ending at $x$.

i.e. there is a vector between origin and A.

If we say that the point at the origin is the point $O(0,0)O(0,0)$ then the vector above is the vector $\vec{OA}$. We could also give it an arbitrary name such as **$u$**.

> Definition: A vector is an object that has both a magnitude and a direction.

### SVM HyperPlane :

#### Understanding the equation of the hyperplane :
* Equation of a line is $ y= mx +c$
* For Hyperplane, it is defined as :

**$$ w^Tx=0$$**

**How do the 2 forms relate?**
In hyperplane equation, we can see that the name of the variables are in bold which means that they are vectors.

Morover, $w^Tx$ is how we compute the inner product of the 2 vectors a.k.a **dot product**.

Note :
$$y= mx +c$$
is the same thing as
$$y-mx-c=0$$

Given 2 vectors,
$$w=\begin{pmatrix} -c \\ -m \\ 1 \\ \end{pmatrix}$$ and $$ x= \begin{pmatrix} 1 \\ x \\ y \\ \end{pmatrix}$$

Thus, the dot product will be
$$w^Tx=-c\times(1)+ (-m)\times x+1\times y$$
$$w^Tx=y-mx-c$$

The 2 equations are just different ways of expressing the same thing.

Note : $w_0$ is $-c$, which means that this value determines the intersection of the line with the vertical axis.
> We use the hyperplane equation because it is then easy to work in more than 2 dimensions

> The vector **$w$** will always be normal to the hyperplane.

This property will come in handy to compute the distance from a point to the hyperplane.

### Computing the distance :
![Distance_g1]({filename}/assets/2017-01-17-SVM-3f1de.png)

To simplify this example, we have set $w_0=0$.

In the above figure, the equation of the hyperplane is :
$$x_2=-2x_1$$

Which is equivalent to

$$w^Tx=0$$

with $$w \begin{pmatrix}2 \\ 1 \\ \end{pmatrix}$$
and $$x\begin{pmatrix} x_1 \\ x_2 \\ \end{pmatrix}$$

**$w$ is a vector**.

#### AIM : To calculate the distance between $A(3,4)$ and its projection onto the hyperplane.

![Distance_g2]({filename}/assets/2017-01-17-SVM-1ef46.png)

We can view the point $A$ as a vector from the origin to $A$.
If we project it onto the normal vector $w$.
![Distance_g3]({filename}/assets/2017-01-17-SVM-8168b.png)

We get the vector $p$ after projection.
![Distance_g3]({filename}/assets/2017-01-17-SVM-00f62.png)

Our goal is to find the distance between the point $A(3,4)$ and the hyperplane.

We can see in the figure that this distance is the same thing as $|p|$
Let's compute this value.

To compute, we need the pre-requisite of Vector Projections :

### Vector Projections

The **vector projection** of a vector $\vec{a}$ on (or onto) a non-zero vector $
\vec{b}$ (a.k.a. the **vector component** or **vector resolution** of $\vec{a}$ in the direction of $\vec{b}$) is the *orthogonal projection* (shadow) of $\vec{a}$ onto a straight line parallel to $\vec{b}$. It is a vector parallel to $\vec{b}$, defined as :

$$\vec{a_1} = a_1\hat{b}$$

where $a_1$ is a scalar, called the **scalar projection of a onto b**, and $\hat{b}$ is the unit vector in the direction of b.

* In turn, the scalar projection is defined as

$$a_1 = |a|cos\theta =\vec{a}.\hat{b}=a.\frac{b}{|b|}$$
where the operator $��$ denotes a dot product, $|a|$ is the length of a, and $\theta$ is the angle between $\vec{a}$ and $\vec{b}$.

The scalar projection is equal to the length of the vector projection, with a minus sign if the direction of the projection is opposite to the direction of b.

## Math for Orthogonal Projection

We start with two vectors, $\vec{w}=(2,1)$ which is normal to the hyperplane, and $\vec{a}=(3,4)$ which is the vector between the origin and $A$.

By Pythagoras theorem, the magnitude of $w$ is :
$$|\vec{w}|=\sqrt{2^2+1^2}$$

Now we take a unit vector $\vec{u}$ in the direction of $\vec{w}$.

Thus $\vec{u}$ becomes :
$$ \vec{u}=(\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}})$$

Given $\vec{p}$ is the orthogonal projection of $\vec{a}$ onto $\vec{w}$ so :

$\vec{p}=(\vec{u}.\vec{a})\vec{u}$

$$\vec{p}=(3\times\frac{2}{\sqrt{5}}+4\times \frac{1}{\sqrt{5}})u$$
 Here, we have calculated the *scalar projection* and now need to multiply it with the unit vector in direction of $\vec{w}$ i.e. :
 $$ p=(\frac{6}{\sqrt{5}}+\frac{4}{\sqrt{5}})u$$

 $$p=\frac{10}{\sqrt{5}}u$$

 $$p=(\frac{10}{\sqrt{5}}\times\frac{2}{\sqrt{5}},\frac{10}{\sqrt{5}}\times\frac{1}{\sqrt{5}})$$

$$p=(4,2)$$

Now, we need the value/magnitude of this projection :

$$|p|=\sqrt{4^2+2^2}=2\sqrt{5}$$

##### Compute the margin of the hyperplane:

Now that we have the distance $|p|$  between $A$ and the hyperplane, the margin is defined by:

$$ margin = 2|p|=4\sqrt{5}$$

## Optimal Margin Hyperplane :
![OMH_1]({filename}/assets/2017-01-17-SVM-e3d86.png)

The above margin is not the biggest/optimal margin. What we need is the optimal margin.

![OMH_2]({filename}/assets/2017-01-17-SVM-28cd5.png).

From the above figure, it's evident that the biggest margin is $M_2$ and not $M_1$.
Thus, the optimal hyperplane is slightly left from our initial hyperplane:

## How to find the biggest margin ?

**Method**:

1. Take a dataset
2. Select 2 hyperplanes which separate the data with no points between them.
3. Maximize their distance (the margin).

> The region bounded by the 2 hyperplanes will be the biggest possible margin.


### Step by Step breakdown :
#### Step 1 : You have a dataset $D$ and you want to classify it :

Most of the time, the data will be composed of $n$ vectors $\vec{x_i}$.

Each $\vec{x_i}$ will also be associated with a value $y_i$ indicating if the element belongs to the class (+1) or not (i.e. -1).

* $y_i$ can only have 2 possible values -1 or +1.

Moreover, most of the time,your vector $\vec{x_i}$ ends up having a lot of dimensions. We can say that $\vec{x_i}$ is a $p$-dimensional vector if it has $p$ dimensions.

* Thus, the dataset $D$ is the set of $n$ couples of element $(\vec{x_i},y_i)$.

The more formal definition of an initial dataset in set theory is:
$$D=\{(\vec{x_i},y_i)\space | \space \vec{x_i} \space \epsilon \space  R^p, \space y_i \space \epsilon \space  \{-1,1\}\}_{i=1}^n$$

#### Step 2  : Select 2 hyperplanes separating the data with no points between them.

Finding 2 hyperplanes separating some data is easy  when you have a pencil and a paper. But with some $p$-dimensional data it becomes more difficult because you can't draw it.

Moreover, even if your data is only 2-dimensional it might not be possible to find a separating hyperplane!

> The classification is only possible when the data is linearly separable.

![OHP_3]({filename}/assets/2017-01-17-SVM-17f74.png)

*Assuming that our dataset $D$ is linearly separable. We now want to find two hyperplanes with no points between them, but we don't have a way to visualize them*.

**Take another look at the hyperplane equation.**

$$w^Tx=0$$
